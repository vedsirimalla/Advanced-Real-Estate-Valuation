# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hh149tx9_vQ1V04CI-0aXXU70arMHd_C

### Loading libraries for the task.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import joblib
pd.set_option('display.max_columns', 81)

"""### Reading train data and dropping unnecessary column Id."""

train = pd.read_csv('train.csv') # read train data
train.drop(['Id'], axis=1, inplace= True) # drop Id column
train.head() # display head

"""### Reading test data and dropping unnecessary column Id."""

test = pd.read_csv('test.csv') # read test data
test.drop(['Id'], axis=1, inplace= True) # drop Id column
test.head() # display head

"""### Data Preprocessing: Dropping columns that have more than 15% of data missing."""

missing_percentage = (train.isnull().sum() / len(train)) * 100 # find the percentage of missing values in data

columns_with_high_missing = missing_percentage[missing_percentage > 15].index.tolist() # find columns having more than 15% missing values

train.drop(columns_with_high_missing,axis=1, inplace= True) # drop these columns from train data
test.drop(columns_with_high_missing,axis=1, inplace= True) # drop these columns from test data

"""### Data Preprocessing:

- Imputing numerical values with the median
- Imputing categorical values with the mode
"""

numeric_columns = train.select_dtypes(include='number').columns.drop('SalePrice') # select numerical features from data
train[numeric_columns] = train[numeric_columns].fillna(train[numeric_columns].median()) # impute train data with median
test[numeric_columns] = test[numeric_columns].fillna(test[numeric_columns].median()) # impute test data with median


categorical_columns = train.select_dtypes(include='object').columns # select categorical features from data
train[categorical_columns] = train[categorical_columns].fillna(train[categorical_columns].mode().iloc[0]) # impute train with mode
test[categorical_columns] = test[categorical_columns].fillna(test[categorical_columns].mode().iloc[0]) # impute test with mode

"""### Exploratory Data Analysis: Plot of GrLivArea with SalePrice."""

plt.figure(figsize=(10, 6))
plt.scatter(train['GrLivArea'], train['SalePrice'], alpha=0.5, color='b') # scatter plot of GrLivArea with SalePrice
plt.title('Scatter Plot of GrLivArea vs SalePrice')
plt.xlabel('GrLivArea')
plt.ylabel('SalePrice')
plt.show()

"""### Two data points show an anomaly where the area is >4000 yet prices are low. We locate them and drop them from the data as they are outliers."""

train[train['GrLivArea']>4000] ## find out the outlier points

"""### Remove outlier points"""

train = train.drop(index=[523, 1298]) # drop outlier points

"""### Exploratory Data Analysis: Distribution of Sales Price."""

sns.histplot(train['SalePrice'], kde=True)
plt.title('Distribution of SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')
plt.show()

"""### Exploratory Data Analysis: Distribution of GrLivArea."""

sns.histplot(train['GrLivArea'], kde=True)
plt.title('Distribution of GrLivArea')
plt.xlabel('GrLivArea')
plt.ylabel('Frequency')
plt.show()

"""### Exploratory Data Analysis: Box Plot of sales price with overall quality."""

sns.boxplot(x='OverallQual', y='SalePrice', data=train)
plt.title('Box Plot of SalePrice vs OverallQual')
plt.show()

"""### Exploratory Data Analysis: Box plot of sales price with overall condition."""

sns.boxplot(x='OverallCond', y='SalePrice', data=train)
plt.title('Box Plot of SalePrice vs OverallCond')
plt.show()

"""### Exploratory Data Analysis: Heatmap of most correlated features."""

numerical_features = train.select_dtypes(include=['int', 'float']) # select numerical features
important_numerical_features = numerical_features.corr()['SalePrice'].abs().nlargest(10).index # select 10 most correlated features
correlation_matrix = train[important_numerical_features].corr()

## Plotting heatmap

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Important Numerical Features')
plt.show()

"""### Data Preprocessing: Label encoding categorical features"""

categorical_features = train.select_dtypes(include=['object']).columns # select categorical features

label_encoder = LabelEncoder()

# Fitting LabelEncoder on training data and transform both training and testing data
for feature in categorical_features:
    # Fit on training data
    label_encoder.fit(train[feature])

    # Transform training data
    train[feature] = label_encoder.transform(train[feature])

    # Transform testing data
    test[feature] = label_encoder.transform(test[feature])

"""### Data Preprocessing: Scaling numerical features."""

numerical_features = train.select_dtypes(include=['int', 'float']).columns # select numerical features


scaler = StandardScaler()

# Fitting StandardScaler on training data and transform both training and testing data
for feature in numerical_features:

    if feature not in ['SalePrice']:
        scaler.fit(train[[feature]])

    # Transform training data
        train[feature] = scaler.transform(train[[feature]])

    # Transform testing data
        test[feature] = scaler.transform(test[[feature]])

"""### Defining the model with the parameter grids."""

models = {
    "Linear Regression": (LinearRegression(), {}),
    "Random Forest": (RandomForestRegressor(), {'n_estimators': [100, 200, 300]}),
    "Gradient Boosting": (GradientBoostingRegressor(), {'n_estimators': [100, 200, 300], 'learning_rate': [0.1, 0.05, 0.01]})
}

"""### Doing training and validation split and declaring variables for storing results."""

test_result = test.copy()

# Splitting training set into training and validation sets with 80-20% ratio.
X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['SalePrice']), train['SalePrice'], test_size=0.2, random_state=42)

## For storing results
results_dict = {'Model': [], 'RMSE': [], 'MAE': []}

test_errors_rmse = {}
test_errors_mae = {}

"""### Performing the ML model training in the following steps:

- Looping each model to be trained
- Performing hyperparameter tuning using grid search with 5 fold CV
- Storing the best model and predicting on validation set.
- Storing the prediction for each model on the test data
- Evaluating the RMSE and MAE for each model
"""

# Looping each model top erform grid search and selecting best model
for model_name, (model, param_grid) in models.items():

    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)

    # Selecting best model
    best_model = grid_search.best_estimator_

    # Training best model on the entire training set
    best_model.fit(X_train, y_train)

    # Predicting on test set
    y_test_pred = best_model.predict(X_val)


    test_predictions = best_model.predict(test)
    test_result['SalesPrice_' + model_name] = test_predictions

    # Test set RMSE and MAE
    test_rmse = mean_squared_error(y_val, y_test_pred, squared=False)
    test_mae = mean_absolute_error(y_val, y_test_pred)

    # Storing test errors
    test_errors_rmse[model_name] = test_rmse
    test_errors_mae[model_name] = test_mae

    # Storing best model
    joblib.dump(best_model, 'best_model_{}.pkl'.format(model_name))

    results_dict['Model'].append(model_name)
    results_dict['RMSE'].append(test_rmse)
    results_dict['MAE'].append(test_mae)


test.to_csv('test_with_predictions.csv', index=False)

"""### Displaying results data."""

results_df = pd.DataFrame(results_dict)
results_df

"""### Plotting histogram of RMSE for all three models"""

plt.figure(figsize=(10, 6))
plt.bar(results_df['Model'], results_df['RMSE'], color='blue', alpha=0.7)
plt.xlabel('Model')
plt.ylabel('RMSE')
plt.title('RMSE for All Models')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""### Plotting histogram of MAE for all three models"""

plt.figure(figsize=(10, 6))
plt.bar(results_df['Model'], results_df['MAE'], color='orange', alpha=0.7)
plt.xlabel('Model')
plt.ylabel('MAE')
plt.title('MAE for All Models')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()